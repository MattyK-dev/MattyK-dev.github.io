{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SemiSupervised_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattyK-dev/MattyK-dev.github.io/blob/master/SemiSupervised_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7iqCtEXHYVX"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0trJmd6DjqBZ"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import math\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Resizing, Rescaling\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Softmax, LeakyReLU\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "%load_ext tensorboard\n",
        "tf.random.set_seed(1)\n",
        "random.seed(1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGXevq7TGexr"
      },
      "source": [
        "Global:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcEy5btVf0eg",
        "outputId": "410a6193-ccf3-47d2-8579-97c85174e2d9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EoD50nJhoNe"
      },
      "source": [
        "# drive.flush_and_unmount()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ektmblmAGrF"
      },
      "source": [
        "def initialise():\n",
        "  # Constants:\n",
        "  global model, unlabelled, batch_size, glob_epoch, loss_object, optimizer, labelled, test, validate, glob_epoch\n",
        "  global train_loss, train_accuracy, validate_loss, validate_accuracy, test_loss, test_accuracy\n",
        "  global train_summary_writer, test_summary_writer, validate_summary_writer\n",
        "  shuffle_buffer_size = 1024\n",
        "  batch_size = 2\n",
        "\n",
        "  ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      'drive/MyDrive/dataset/pestsAndDiseases/',\n",
        "      labels='inferred',\n",
        "      label_mode = \"int\",\n",
        "      color_mode='rgb',\n",
        "      batch_size=batch_size,\n",
        "      # image_size=(img_height, img_width),\n",
        "      shuffle=True,\n",
        "      seed=1,\n",
        "  )\n",
        "\n",
        "  # datagen = ImageDataGenerator(\n",
        "  #     rescale=1./255,\n",
        "  #     rotation_range=5,\n",
        "  #     zoom_range=0.95, 0.95,\n",
        "  #     horizontal_flip=True,\n",
        "  #     vertical_flip=True,\n",
        "  #     data_format='channels_last',\n",
        "  #     validation_split=0.0,\n",
        "  #     dtype=tf.float32\n",
        "  # )\n",
        "\n",
        "  # train_generator = datagen.flow_from_directory(\n",
        "  #     'drive/MyDrive/dataset/pestsAndDiseases/',\n",
        "  #     batch_size=batch_size,\n",
        "  #     color_mode='rgb',\n",
        "  #     class_mode='sparse',\n",
        "  #     shuffle=True,\n",
        "  #     subset=training\n",
        "  # )\n",
        "\n",
        "  # Split the dataset into labelled, unlabelled, validate and test\n",
        "  ds = ds.shuffle(shuffle_buffer_size)\n",
        "  dataset_size = len(ds)\n",
        "  labelled = ds.take(int(0.1 * dataset_size))\n",
        "  ds = ds.skip(int(0.1 * dataset_size))\n",
        "  unlabelled = ds.take(int(0.6 * dataset_size))\n",
        "  ds = ds.skip(int(0.6 * dataset_size))\n",
        "  validate = ds.take(int(0.2 * dataset_size))\n",
        "  ds = ds.skip(int(0.2 * dataset_size))\n",
        "  test = ds.take(int(0.1 * dataset_size))\n",
        "  ds = ds.skip(int(0.1 * dataset_size))\n",
        "\n",
        "  # Shuffle dataset\n",
        "  # Use `tf.data` to batch and shuffle the dataset:\n",
        "  labelled = labelled.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  unlabelled = unlabelled.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  validate = validate.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  test = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Choose an optimizer and loss function for training: \n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  # Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\n",
        "  train_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='train_loss')\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "  validate_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='validate_loss')\n",
        "  validate_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='validate_accuracy')\n",
        "  test_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='test_loss')\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "  # Tensorboard configurations\n",
        "  current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "  train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "  validate_log_dir = 'logs/gradient_tape/' + current_time + '/validate'\n",
        "  test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "\n",
        "  train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "  validate_summary_writer = tf.summary.create_file_writer(validate_log_dir)\n",
        "  test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibvyrfbUBsVn"
      },
      "source": [
        "Train, validate and test the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZACiVqA8KQV"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images, training=True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(labels, predictions)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def validate_step(images, labels):\n",
        "  predictions = model(images, training=False)\n",
        "  v_loss = loss_object(labels, predictions)\n",
        "\n",
        "  validate_loss(labels, predictions)\n",
        "  validate_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  predictions = model(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss(labels, predictions)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmwfRTBLChWO"
      },
      "source": [
        "Define the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    # self.resize = Resizing(64, 64)\n",
        "    self.rescale = Rescaling(1.0/255)\n",
        "    self.conv = Conv2D(8, 8)\n",
        "    self.lr1 = LeakyReLU()\n",
        "    self.conv2 = Conv2D(16, 4)\n",
        "    self.lr2 = LeakyReLU()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128)\n",
        "    self.lr3 = LeakyReLU()\n",
        "    self.d2 = Dense(10)\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    # x = self.resize(x)\n",
        "    x = self.rescale(x)\n",
        "    x = self.conv(x)\n",
        "    x = self.lr1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.lr2(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.lr3(x)\n",
        "    x = self.d2(x)\n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-2pkctU_Ci7"
      },
      "source": [
        "# Step 1: train model on provided labelled data(initial data)\n",
        "# Step 2: Train Semi-supervised learning layer(Use the new labels to iteratively train the model)\n",
        "def modelTrain(EPOCHS):\n",
        "  global model, unlabelled, batch_size, glob_epoch, loss_object, optimizer, labelled, test, validate, glob_epoch\n",
        "  global train_loss, train_accuracy, validate_loss, validate_accuracy, test_loss, test_accuracy\n",
        "  global train_summary_writer, test_summary_writer, validate_summary_writer, batch_size\n",
        "  progress = tf.keras.utils.Progbar(EPOCHS, width=30, verbose=1, interval=0.05, stateful_metrics=['train_loss', 'train_accuracy', 'test_loss', 'test_accuracy'], unit_name='step')\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    # progress.update(epoch+1)\n",
        "    glob_epoch += 1\n",
        "    \n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    validate_loss.reset_states()\n",
        "    validate_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    # Step 1: train model on labelled data\n",
        "    for images, labels in labelled:\n",
        "      train_step(images, labels)\n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.scalar('Train loss', train_loss.result(), step=glob_epoch)\n",
        "      tf.summary.scalar('Train accuracy', train_accuracy.result(), step=glob_epoch)\n",
        "\n",
        "    # Validate and test\n",
        "    for images, labels in validate:\n",
        "      validate_step(images, labels)\n",
        "    with validate_summary_writer.as_default():\n",
        "      tf.summary.scalar('Validation loss', validate_loss.result(), step=glob_epoch)\n",
        "      tf.summary.scalar('Validation accuracy', validate_accuracy.result(), step=glob_epoch)\n",
        "\n",
        "    for test_images, test_labels in test:\n",
        "      test_step(test_images, test_labels)\n",
        "    with test_summary_writer.as_default():\n",
        "      tf.summary.scalar('Test loss', test_loss.result(), step=glob_epoch)\n",
        "      tf.summary.scalar('Test accuracy', test_accuracy.result(), step=glob_epoch)\n",
        "\n",
        "    print(\n",
        "      f'Epoch {epoch + 1}, '\n",
        "      f'Loss: {train_loss.result()}, '\n",
        "      f'Accuracy: {train_accuracy.result() * 100}, '\n",
        "      f'Validation Loss: {validate_loss.result()}, '\n",
        "      f'Validation Accuracy: {validate_accuracy.result() * 100}, '\n",
        "      f'Test Loss: {test_loss.result()}, '\n",
        "      f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "    )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n72l_DqSwBxx"
      },
      "source": [
        "def pseudo_labelling(images, labels):\n",
        "  labels = tf.math.argmax(model(images), axis = 1, output_type=tf.dtypes.int32)\n",
        "  return images, labels\n",
        "\n",
        "def pseudo_subset(subset_size):\n",
        "  global model, unlabelled, batch_size, glob_epoch, loss_object, optimizer, labelled, test, validate\n",
        "  global train_loss, train_accuracy, validate_loss, validate_accuracy, test_loss, test_accuracy\n",
        "  global train_summary_writer, test_summary_writer, validate_summary_writer, batch_size\n",
        "  subset = unlabelled.take(subset_size)\n",
        "  unlabelled = unlabelled.skip(subset_size)\n",
        "  subset = subset.map(pseudo_labelling)\n",
        "  labelled = labelled.concatenate(subset)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqL0LoMTQ-Mc"
      },
      "source": [
        "Main:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2sQRaXNEmF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810036bb-f877-4c45-8a45-993044081a78"
      },
      "source": [
        "def main():\n",
        "  global model, unlabelled, batch_size, glob_epoch, loss_object, optimizer, labelled, test, validate\n",
        "  global train_loss, train_accuracy, validate_loss, validate_accuracy, test_loss, test_accuracy\n",
        "  global train_summary_writer, test_summary_writer, validate_summary_writer, batch_size\n",
        "  #constants\n",
        "  glob_epoch = 0\n",
        "  repeat_training = 1\n",
        "  training_EPOCHS = 8\n",
        "  subset_size = 5\n",
        "\n",
        "  for i in range(repeat_training):\n",
        "    initialise()\n",
        "    # Create an instance of the model\n",
        "    model = MyModel()\n",
        "  \n",
        "    # Size of subsets divided by batch size:\n",
        "    iterations = math.floor(len(unlabelled) / subset_size)\n",
        "\n",
        "    for j in range(iterations):\n",
        "      # Train the model on the few available labelled data\n",
        "      modelTrain(training_EPOCHS)\n",
        "      pseudo_subset(subset_size)\n",
        "\n",
        "    model.save('logs/models')\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2378 files belonging to 10 classes.\n",
            "Epoch 1, Loss: 8.563268661499023, Accuracy: 3.125, Validation Loss: 5.971063137054443, Validation Accuracy: 21.66666603088379, Test Loss: 5.056022644042969, Test Accuracy: 27.23214340209961\n",
            "Epoch 2, Loss: 3.8546395301818848, Accuracy: 21.875, Validation Loss: 4.7524638175964355, Validation Accuracy: 21.875, Test Loss: 5.494017124176025, Test Accuracy: 17.85714340209961\n",
            "Epoch 3, Loss: 4.717474460601807, Accuracy: 25.0, Validation Loss: 8.02916145324707, Validation Accuracy: 16.15720558166504, Test Loss: 8.359411239624023, Test Accuracy: 20.089284896850586\n",
            "Epoch 4, Loss: 10.28063678741455, Accuracy: 15.625, Validation Loss: 10.190685272216797, Validation Accuracy: 10.416666030883789, Test Loss: 11.184470176696777, Test Accuracy: 8.482142448425293\n",
            "Epoch 5, Loss: 9.183225631713867, Accuracy: 3.125, Validation Loss: 7.819547176361084, Validation Accuracy: 23.958332061767578, Test Loss: 7.877242565155029, Test Accuracy: 28.571430206298828\n",
            "Epoch 6, Loss: 6.341696262359619, Accuracy: 25.0, Validation Loss: 6.093394756317139, Validation Accuracy: 23.75, Test Loss: 7.23412561416626, Test Accuracy: 22.321428298950195\n",
            "Epoch 7, Loss: 4.936603546142578, Accuracy: 37.5, Validation Loss: 4.59581184387207, Validation Accuracy: 21.04166603088379, Test Loss: 5.012059211730957, Test Accuracy: 22.76785659790039\n",
            "Epoch 8, Loss: 6.156674385070801, Accuracy: 6.25, Validation Loss: 12.604985237121582, Validation Accuracy: 12.083333015441895, Test Loss: 12.206048965454102, Test Accuracy: 9.821428298950195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1amx6aZ002P"
      },
      "source": [
        "# !rm -r logs\n",
        "%tensorboard --logdir logs/gradient_tape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whQrfjFBwDRR"
      },
      "source": [
        "# !zip -r /runa.zip /content/logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}