{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OneShot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzswxEnfevdyb+4bJD52lp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattyK-dev/MattyK-dev.github.io/blob/AI/OneShot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZUpHYB1LvFP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import wandb\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Softmax\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "%load_ext tensorboard\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYIHgaaDNXbl"
      },
      "source": [
        "# load data\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# x_train = x_train.astype('float32')\n",
        "# x_test = x_test.astype('float32')\n",
        "# x_train /= 255\n",
        "# x_test /= 255\n",
        "\n",
        "def initialise():\n",
        "  global train, validate, test, loss_object, optimizer\n",
        "  global train_loss, train_accuracy, validate_loss, validate_accuracy, test_loss, test_accuracy\n",
        "  global train_summary_writer, test_summary_writer, validate_summary_writer, batch_size\n",
        "\n",
        "  # Constants:\n",
        "  batch_size = 64\n",
        "  shuffle_buffer_size = 1024\n",
        "\n",
        "  # Split the dataset into labelled, unlabelled, validate and test\n",
        "  train, validate, test = tfds.load('plant_village', split=['train[:10%]', 'train[90%:95%]', 'train[95%:]'], shuffle_files=True, as_supervised=True)\n",
        "\n",
        "  # Shuffle dataset\n",
        "  # Use `tf.data` to batch and shuffle the dataset:\n",
        "  train = train.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  validate = validate.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  test = test.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Choose an optimizer and loss function for training: \n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  # Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\n",
        "  train_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='train_loss')\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "  validate_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='validate_loss')\n",
        "  validate_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='validate_accuracy')\n",
        "\n",
        "  test_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='test_loss')\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "  \n",
        "  # Tensorboard configurations\n",
        "  current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  \n",
        "  train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "  validate_log_dir = 'logs/gradient_tape/' + current_time + '/validate'\n",
        "  test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "\n",
        "  train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "  validate_summary_writer = tf.summary.create_file_writer(validate_log_dir)\n",
        "  test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-QXgP9-NdjW"
      },
      "source": [
        "# make pairs\n",
        "def make_pairs(x, y):\n",
        "    num_classes = max(y) + 1\n",
        "    digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
        "\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for idx1 in range(len(x)):\n",
        "        # add a matching example\n",
        "        x1 = x[idx1]\n",
        "        label1 = y[idx1]\n",
        "        idx2 = random.choice(digit_indices[label1])\n",
        "        x2 = x[idx2]\n",
        "        \n",
        "        pairs += [[x1, x2]]\n",
        "        labels += [1]\n",
        "    \n",
        "        # add a not matching example\n",
        "        label2 = random.randint(0, num_classes-1)\n",
        "        while label2 == label1:\n",
        "            label2 = random.randint(0, num_classes-1)\n",
        "\n",
        "        idx2 = random.choice(digit_indices[label2])\n",
        "        x2 = x[idx2]\n",
        "        \n",
        "        pairs += [[x1, x2]]\n",
        "        labels += [0]\n",
        "\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "pairs_train, labels_train = make_pairs(x_train, y_train)\n",
        "pairs_test, labels_test = make_pairs(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep2sbpmLjIS6"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  # For Training\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images, training=True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(labels, predictions)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def validate_step(images, labels):\n",
        "  # for Validation\n",
        "  predictions = model(images, training=True)\n",
        "  v_loss = loss_object(labels, predictions)\n",
        "\n",
        "  validate_loss(labels, predictions)\n",
        "  validate_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  predictions = model(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss(labels, predictions)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i_WmI7VNoCh"
      },
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
        "\n",
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.resize = Resizing(64, 64)\n",
        "    self.rescale = Rescaling(1.0/255)\n",
        "    self.conv = Conv2D(8, 8, activation='relu')\n",
        "    self.conv2 = Conv2D(16, 4, activation='relu')\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128, activation='relu')\n",
        "    self.d2 = Dense(38)\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    x = self.resize(x)\n",
        "    x = self.rescale(x)\n",
        "    x = self.conv(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    if not training:\n",
        "      x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "model = MyModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h053OG07Npan"
      },
      "source": [
        "wandb.init(project=\"siamese\")\n",
        "model.fit([pairs_train[:,0], pairs_train[:,1]], labels_train[:], batch_size=16, epochs= 10, callbacks=[WandbCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTzeyY2Nywr"
      },
      "source": [
        "\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_COb4K0N3xJ"
      },
      "source": [
        "wandb.init(project=\"siamese\")\n",
        "model.fit([pairs_train[:,0], pairs_train[:,1]], labels_train[:], batch_size=16, epochs=10, callbacks=[WandbCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxZZbZxvjrJl"
      },
      "source": [
        "# Old: \n",
        "# # Weights not shared\n",
        "\n",
        "# seq1 = Sequential()\n",
        "# seq1.add(Flatten(input_shape=(28,28)))\n",
        "# seq1.add(Dense(128, activation='relu'))\n",
        "\n",
        "# seq2 = Sequential()\n",
        "# seq2.add(Flatten(input_shape=(28,28)))\n",
        "# seq2.add(Dense(128, activation='relu'))\n",
        "\n",
        "# merge_layer = Concatenate()([seq1.output, seq2.output])\n",
        "# dense_layer = Dense(1, activation=\"sigmoid\")(merge_layer)\n",
        "# model = Model(inputs=[seq1.input, seq2.input], outputs=dense_layer)\n",
        "\n",
        "# input = Input((28,28))\n",
        "# x = Flatten()(input)\n",
        "# x = Dense(128, activation='relu')(x)\n",
        "# dense = Model(input, x)\n",
        "\n",
        "# input1 = Input((28,28))\n",
        "# input2 = Input((28,28))\n",
        "\n",
        "# dense1 = dense(input1)\n",
        "# dense2 = dense(input2)\n",
        "\n",
        "# merge_layer = Concatenate()([dense1, dense2])\n",
        "# dense_layer = Dense(1, activation=\"sigmoid\")(merge_layer)\n",
        "# model = Model(inputs=[input1, input2], outputs=dense_layer)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}